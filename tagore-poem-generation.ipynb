{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Generating Shakespearean Text with Character Based RNNs\n\nProblem Statement: Given a character or sequence of characters, we want to predict the next character at each time step. Model is trained to follow a language similar to the works of Shakespeare. The tinyshakespear dataset is used for training.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport os\nimport time","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2023-03-05T06:23:05.711674Z","iopub.execute_input":"2023-03-05T06:23:05.712011Z","iopub.status.idle":"2023-03-05T06:23:05.717005Z","shell.execute_reply.started":"2023-03-05T06:23:05.711980Z","shell.execute_reply":"2023-03-05T06:23:05.715911Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#check if decoding is needed: text may need to be decoded as utf-8\ntext = open('/kaggle/input/complete-works-of-rabindranath-tagore/txt/poem.txt', 'r').read() \nprint(text[:200])","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:05.718826Z","iopub.execute_input":"2023-03-05T06:23:05.719415Z","iopub.status.idle":"2023-03-05T06:23:05.752349Z","shell.execute_reply.started":"2023-03-05T06:23:05.719364Z","shell.execute_reply":"2023-03-05T06:23:05.751488Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"বজাও রে মোহন বাঁশি।\nসারা দিবসক\nবিরহদহনদুখ,\nমরমক তিয়াষ নাশি।\nরিঝমনভেদন\nবাঁশরিবাদন\nকঁহা শিখলি রে কান?\nহানে থিরথির\nমরমঅবশকর\nলহু লহু মধুময় বাণ।\nধসধস করতহ\nউরহ বিয়াকুলু,\nঢুলু ঢুলু অবশনয়ান ;\nকত কত বরষক\nবাত স\n","output_type":"stream"}]},{"cell_type":"code","source":"#Find Vocabulary (set of characters)\nvocabulary = sorted(set(text))\nprint('No. of unique characters: {}'.format(len(vocabulary)))","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:05.754532Z","iopub.execute_input":"2023-03-05T06:23:05.754908Z","iopub.status.idle":"2023-03-05T06:23:05.880022Z","shell.execute_reply.started":"2023-03-05T06:23:05.754868Z","shell.execute_reply":"2023-03-05T06:23:05.879132Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"No. of unique characters: 139\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preprocessing Text","metadata":{}},{"cell_type":"code","source":"#character to index mapping\nchar2index = {c:i for i,c in enumerate(vocabulary)}\nint_text = np.array([char2index[i] for i in text])\n\n#Index to character mapping\nindex2char = np.array(vocabulary)","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:05.882020Z","iopub.execute_input":"2023-03-05T06:23:05.882622Z","iopub.status.idle":"2023-03-05T06:23:06.305495Z","shell.execute_reply.started":"2023-03-05T06:23:05.882566Z","shell.execute_reply":"2023-03-05T06:23:06.304703Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#Testing\nprint(\"Character to Index: \\n\")\nfor char,_ in zip(char2index, range(65)):\n    print('  {:4s}: {:3d}'.format(repr(char), char2index[char]))\n\nprint(\"\\nInput text to Integer: \\n\")\nprint('{} mapped to {}'.format(repr(text[:20]),int_text[:20])) #use repr() for debugging","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:06.306980Z","iopub.execute_input":"2023-03-05T06:23:06.307455Z","iopub.status.idle":"2023-03-05T06:23:06.319864Z","shell.execute_reply.started":"2023-03-05T06:23:06.307417Z","shell.execute_reply":"2023-03-05T06:23:06.318893Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Character to Index: \n\n  '\\n':   0\n  ' ' :   1\n  '!' :   2\n  '\"' :   3\n  \"'\" :   4\n  '(' :   5\n  ')' :   6\n  ',' :   7\n  '-' :   8\n  '.' :   9\n  '1' :  10\n  '2' :  11\n  '6' :  12\n  '7' :  13\n  '9' :  14\n  ':' :  15\n  ';' :  16\n  '?' :  17\n  'B' :  18\n  'C' :  19\n  'F' :  20\n  'H' :  21\n  'J' :  22\n  'L' :  23\n  'M' :  24\n  'N' :  25\n  'O' :  26\n  'R' :  27\n  'T' :  28\n  'W' :  29\n  '[' :  30\n  ']' :  31\n  '_' :  32\n  'a' :  33\n  'b' :  34\n  'c' :  35\n  'd' :  36\n  'e' :  37\n  'f' :  38\n  'g' :  39\n  'h' :  40\n  'i' :  41\n  'k' :  42\n  'l' :  43\n  'm' :  44\n  'n' :  45\n  'o' :  46\n  'p' :  47\n  'r' :  48\n  's' :  49\n  't' :  50\n  'u' :  51\n  'v' :  52\n  'w' :  53\n  'x' :  54\n  'y' :  55\n  '|' :  56\n  'ű' :  57\n  '̶' :  58\n  '।' :  59\n  '॥' :  60\n  'ঁ' :  61\n  'ং' :  62\n  'ঃ' :  63\n  'অ' :  64\n\nInput text to Integer: \n\n'বজাও রে মোহন বাঁশি।\\n' mapped to [ 97  82 107  73   1 101 113   1  99 115 106  94   1  97 107  61 103 108\n  59   0]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Create Training Data","metadata":{}},{"cell_type":"code","source":"seq_length= 150 #max number of characters that can be fed as a single input\nexamples_per_epoch = len(text)\n\n#converts text (vector) into character index stream\n#Reference: https://www.tensorflow.org/api_docs/python/tf/data/Dataset\nchar_dataset = tf.data.Dataset.from_tensor_slices(int_text)","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:06.321619Z","iopub.execute_input":"2023-03-05T06:23:06.322069Z","iopub.status.idle":"2023-03-05T06:23:06.337358Z","shell.execute_reply.started":"2023-03-05T06:23:06.322023Z","shell.execute_reply":"2023-03-05T06:23:06.336665Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#Create sequences from the individual characters. Our required size will be seq_length + 1 (character RNN)\nsequences = char_dataset.batch(seq_length+1, drop_remainder=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:06.338681Z","iopub.execute_input":"2023-03-05T06:23:06.338997Z","iopub.status.idle":"2023-03-05T06:23:06.350499Z","shell.execute_reply.started":"2023-03-05T06:23:06.338962Z","shell.execute_reply":"2023-03-05T06:23:06.349646Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#Testing\nprint(\"Character Stream: \\n\")\nfor i in char_dataset.take(10):\n  print(index2char[i.numpy()])  \n\nprint(\"\\nSequence: \\n\")\nfor i in sequences.take(10):\n  print(repr(''.join(index2char[i.numpy()])))  #use repr() for more clarity. str() keeps formatting it","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:06.354195Z","iopub.execute_input":"2023-03-05T06:23:06.354487Z","iopub.status.idle":"2023-03-05T06:23:06.396528Z","shell.execute_reply.started":"2023-03-05T06:23:06.354460Z","shell.execute_reply":"2023-03-05T06:23:06.394960Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Character Stream: \n\nব\nজ\nা\nও\n \nর\nে\n \nম\nো\n\nSequence: \n\n'বজাও রে মোহন বাঁশি।\\nসারা দিবসক\\nবিরহদহনদুখ,\\nমরমক তিয়াষ নাশি।\\nরিঝমনভেদন\\nবাঁশরিবাদন\\nকঁহা শিখলি রে কান?\\nহানে থিরথির\\nমরমঅবশকর\\nলহু লহু মধুময় বাণ।\\nধসধস করতহ\\nউ'\n'রহ বিয়াকুলু,\\nঢুলু ঢুলু অবশনয়ান ;\\nকত কত বরষক\\nবাত সোঁয়ারয়,\\nঅধীর করয় পরান।\\nকত শত আশা\\nপূরল না বঁধু,\\nকত সুখ করল পয়ান।\\nপহু গো কত শত\\nপীরিতযাতন\\nহিয়ে বিঁধাওল বা'\n'ণ।\\nহৃদয় উদাসয়,\\nনয়ন উছাসয়\\nদারুণ\\nমধুময় গান।\\nসাধ যায় বঁধূ,\\nযমুনাবারিম\\nডারিব\\nদগধপরান।\\nসাধ যায় পহু,\\nরাখি চরণ তব\\nহৃদয়মাঝ\\nহৃদয়েশ,\\nহৃদয়জুড়াওন\\nবদনচন্দ্র তব\\nহেরব'\n'\\nজীবনশেষ।\\nসাধ যায়, ইহ\\nচন্দ্রমকিরণে\\nকুসুমিত\\nকুঞ্জবিতানে\\nবসন্তবায়ে\\nপ্রাণ মিশায়ব\\nবাঁশিক সুমধুর গানে।\\nপ্রাণ ভৈবে মঝু\\nবেণুগীতময়,\\nরাধাময় তব\\nবেণু।\\nজয় জয় মাধব,'\n'\\nজয় জয় রাধা,\\nচরণে\\nপ্রণমে ভানু।\\nশুনহ শুনহ বালিকা,\\nরাখ কুসুমমালিকা,\\nকুঞ্জ কুঞ্জ ফেরনু সখি শ্যামচন্দ্র নাহি রে।\\nদুলই কুসুমমুঞ্জরী,\\nভমর ফিরই গুঞ্জরি,\\nঅলস য'\n'মুনা বহয়ি যায় ললিত গীত গাহি রে।\\nশশিসনাথ যামিনী,\\nবিরহবিধুর কামিনী,\\nকুসুমহার ভইল ভার— হৃদয় তার দাহিছে।\\nঅধর উঠই কাঁপিয়া\\nসখিকরে কর আপিয়া,\\nকুঞ্জভবনে পাপিয়া '\n'কাহে গীত গাহিছে।\\nমৃদু সমীর সঞ্চলে\\nহরয়ি শিথিল অঞ্চলে,\\nচকিত হৃদয় চঞ্চলে কাননপথ চাহি রে।\\nকুঞ্জপানে হেরিয়া\\nঅশ্রুবারি ডারিয়া\\nভানু গায় শূন্যকুঞ্জ শ্যামচন্দ্র'\n' নাহি রে!\\nহৃদয়ক সাধ মিশাওল হৃদয়ে,\\nকন্ঠে বিমলিন মালা।\\nবিরহবিষে দহি বহি গল রয়নী,\\nনহি নহি আওল কালা।\\nবুঝনু বুঝনু সখি বিফল বিফল সব,\\nবিফল এ পীরিতি লেহা —\\nবিফ'\n'ল রে এ মঝু জীবন যৌবন,\\nবিফল রে এ মঝু দেহা!\\nচল সখি গৃহ চল, মঞ্চ নয়নজল,\\nচল সখি চল গৃহকাজে,\\nমালতিমালা রাখহ বালা,\\nছি ছি সখি মরু মরু লাজে।\\nসখি লো দারুণ আধিভর'\n'াতুর\\nএ তরুণ যৌবন মোর,\\nসখি লো দারুণ প্রণয়হলাহল\\nজীবন করল অঘোর।\\nতৃষিত প্রাণ মম দিবসযামিনী\\nশ্যামক দরশন আশে,\\nআকুল জীবন থেহ ন মানে,\\nঅহরহ জ্বলত হুতাশে।\\nসজনি, '\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\nTarget value: for each sequence of characters, we return that sequence, shifted one position to the right, along with the new character that is predicted to follow the sequence.\n\nTo create training examples of (input, target) pairs, we take the given sequence. The input is sequence with last word removed. Target is sequence with first word removed. Example: sequence: abc d ef input: abc d e target: bc d ef","metadata":{}},{"cell_type":"code","source":"def create_input_target_pair(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(create_input_target_pair)","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:06.398983Z","iopub.execute_input":"2023-03-05T06:23:06.399321Z","iopub.status.idle":"2023-03-05T06:23:06.416129Z","shell.execute_reply.started":"2023-03-05T06:23:06.399283Z","shell.execute_reply":"2023-03-05T06:23:06.415414Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#Testing\nfor input_example, target_example in  dataset.take(1):\n  print ('Input data: ', repr(''.join(index2char[input_example.numpy()])))\n  print ('Target data:', repr(''.join(index2char[target_example.numpy()])))","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:06.419208Z","iopub.execute_input":"2023-03-05T06:23:06.419455Z","iopub.status.idle":"2023-03-05T06:23:06.443058Z","shell.execute_reply.started":"2023-03-05T06:23:06.419430Z","shell.execute_reply":"2023-03-05T06:23:06.442143Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Input data:  'বজাও রে মোহন বাঁশি।\\nসারা দিবসক\\nবিরহদহনদুখ,\\nমরমক তিয়াষ নাশি।\\nরিঝমনভেদন\\nবাঁশরিবাদন\\nকঁহা শিখলি রে কান?\\nহানে থিরথির\\nমরমঅবশকর\\nলহু লহু মধুময় বাণ।\\nধসধস করতহ\\n'\nTarget data: 'জাও রে মোহন বাঁশি।\\nসারা দিবসক\\nবিরহদহনদুখ,\\nমরমক তিয়াষ নাশি।\\nরিঝমনভেদন\\nবাঁশরিবাদন\\nকঁহা শিখলি রে কান?\\nহানে থিরথির\\nমরমঅবশকর\\nলহু লহু মধুময় বাণ।\\nধসধস করতহ\\nউ'\n","output_type":"stream"}]},{"cell_type":"code","source":"#Creating batches\n\nBATCH_SIZE = 64\n\n# Buffer used to shuffle the dataset \n# Reference: https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:06.444357Z","iopub.execute_input":"2023-03-05T06:23:06.444764Z","iopub.status.idle":"2023-03-05T06:23:06.453867Z","shell.execute_reply.started":"2023-03-05T06:23:06.444721Z","shell.execute_reply":"2023-03-05T06:23:06.452721Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"<BatchDataset shapes: ((64, 150), (64, 150)), types: (tf.int64, tf.int64)>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Building the Model","metadata":{}},{"cell_type":"code","source":"vocab_size = len(vocabulary)\nembedding_dim = 256\nrnn_units= 1024","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:06.455352Z","iopub.execute_input":"2023-03-05T06:23:06.455847Z","iopub.status.idle":"2023-03-05T06:23:06.460620Z","shell.execute_reply.started":"2023-03-05T06:23:06.455809Z","shell.execute_reply":"2023-03-05T06:23:06.459694Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"3 Layers used:\n\nInput Layer: Maps character to 256 dimension vector\n\nGRU Layer: RNN of size 1024\n\nDense Layer: Output with same size as vocabulary\n\nSince it is a character level RNN, we can use keras.Sequential model (All layers have single input and single output).","metadata":{}},{"cell_type":"code","source":"def build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size):\n    model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.LSTM(rnn_units, \n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n    return model\n\n# Reference for theory: https://jhui.github.io/2017/03/15/RNN-LSTM-GRU/","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:06.462217Z","iopub.execute_input":"2023-03-05T06:23:06.462651Z","iopub.status.idle":"2023-03-05T06:23:06.470874Z","shell.execute_reply.started":"2023-03-05T06:23:06.462614Z","shell.execute_reply":"2023-03-05T06:23:06.470069Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"lstm_model = build_model_lstm(\n  vocab_size = vocab_size,\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:06.472136Z","iopub.execute_input":"2023-03-05T06:23:06.472549Z","iopub.status.idle":"2023-03-05T06:23:06.695261Z","shell.execute_reply.started":"2023-03-05T06:23:06.472504Z","shell.execute_reply":"2023-03-05T06:23:06.694515Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"#Testing: shape\nfor input_example_batch, target_example_batch in dataset.take(1):\n    example_prediction = lstm_model(input_example_batch)\n    assert (example_prediction.shape == (BATCH_SIZE, seq_length, vocab_size)), \"Shape error\"\n    #print(example_prediction.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:06.697734Z","iopub.execute_input":"2023-03-05T06:23:06.698236Z","iopub.status.idle":"2023-03-05T06:23:10.892281Z","shell.execute_reply.started":"2023-03-05T06:23:06.698196Z","shell.execute_reply":"2023-03-05T06:23:10.891451Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"#model.summary() \n#check shapes if necessary","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:10.896091Z","iopub.execute_input":"2023-03-05T06:23:10.896365Z","iopub.status.idle":"2023-03-05T06:23:10.900700Z","shell.execute_reply.started":"2023-03-05T06:23:10.896338Z","shell.execute_reply":"2023-03-05T06:23:10.899837Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"sampled_indices = tf.random.categorical(example_prediction[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:10.902140Z","iopub.execute_input":"2023-03-05T06:23:10.902494Z","iopub.status.idle":"2023-03-05T06:23:10.918640Z","shell.execute_reply.started":"2023-03-05T06:23:10.902452Z","shell.execute_reply":"2023-03-05T06:23:10.917798Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"def loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\n#Loss Function reference: https://www.dlology.com/blog/how-to-use-keras-sparse_categorical_crossentropy/\n\nexample_loss  = loss(target_example_batch, example_prediction)\nprint(\"Prediction shape: \", example_prediction.shape)\nprint(\"Loss:      \", example_loss.numpy().mean())","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:10.920957Z","iopub.execute_input":"2023-03-05T06:23:10.921328Z","iopub.status.idle":"2023-03-05T06:23:10.936499Z","shell.execute_reply.started":"2023-03-05T06:23:10.921287Z","shell.execute_reply":"2023-03-05T06:23:10.935660Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Prediction shape:  (64, 150, 139)\nLoss:       4.935026\n","output_type":"stream"}]},{"cell_type":"code","source":"lstm_model.compile(optimizer='adam', loss=loss)","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:10.938140Z","iopub.execute_input":"2023-03-05T06:23:10.938496Z","iopub.status.idle":"2023-03-05T06:23:10.953353Z","shell.execute_reply.started":"2023-03-05T06:23:10.938456Z","shell.execute_reply":"2023-03-05T06:23:10.952374Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"lstm_dir_checkpoints= './training_checkpoints_LSTM'\ncheckpoint_prefix = os.path.join(lstm_dir_checkpoints, \"checkpt_{epoch}\") #name\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:10.956559Z","iopub.execute_input":"2023-03-05T06:23:10.956850Z","iopub.status.idle":"2023-03-05T06:23:10.963735Z","shell.execute_reply.started":"2023-03-05T06:23:10.956822Z","shell.execute_reply":"2023-03-05T06:23:10.963079Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"EPOCHS=60 #increase number of epochs for better results (lesser loss)","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:10.965029Z","iopub.execute_input":"2023-03-05T06:23:10.965419Z","iopub.status.idle":"2023-03-05T06:23:10.972429Z","shell.execute_reply.started":"2023-03-05T06:23:10.965375Z","shell.execute_reply":"2023-03-05T06:23:10.971679Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"history = lstm_model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:23:10.973946Z","iopub.execute_input":"2023-03-05T06:23:10.974474Z","iopub.status.idle":"2023-03-05T06:45:26.028804Z","shell.execute_reply.started":"2023-03-05T06:23:10.974437Z","shell.execute_reply":"2023-03-05T06:45:26.028085Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Epoch 1/60\n245/245 [==============================] - 21s 84ms/step - loss: 2.8583\nEpoch 2/60\n245/245 [==============================] - 21s 84ms/step - loss: 2.3891\nEpoch 3/60\n245/245 [==============================] - 21s 84ms/step - loss: 2.1671\nEpoch 4/60\n245/245 [==============================] - 21s 84ms/step - loss: 2.0206\nEpoch 5/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.9280\nEpoch 6/60\n245/245 [==============================] - 20s 84ms/step - loss: 1.8658\nEpoch 7/60\n245/245 [==============================] - 21s 84ms/step - loss: 1.8206\nEpoch 8/60\n245/245 [==============================] - 20s 84ms/step - loss: 1.7857\nEpoch 9/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.7564\nEpoch 10/60\n245/245 [==============================] - 20s 84ms/step - loss: 1.7307\nEpoch 11/60\n245/245 [==============================] - 21s 84ms/step - loss: 1.7082\nEpoch 12/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.6870\nEpoch 13/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.6675\nEpoch 14/60\n245/245 [==============================] - 21s 84ms/step - loss: 1.6485\nEpoch 15/60\n245/245 [==============================] - 21s 84ms/step - loss: 1.6306\nEpoch 16/60\n245/245 [==============================] - 21s 84ms/step - loss: 1.6131\nEpoch 17/60\n245/245 [==============================] - 21s 84ms/step - loss: 1.5963\nEpoch 18/60\n245/245 [==============================] - 21s 84ms/step - loss: 1.5783\nEpoch 19/60\n245/245 [==============================] - 20s 84ms/step - loss: 1.5620\nEpoch 20/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.5448\nEpoch 21/60\n245/245 [==============================] - 21s 84ms/step - loss: 1.5274\nEpoch 22/60\n245/245 [==============================] - 20s 84ms/step - loss: 1.5097\nEpoch 23/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.4930\nEpoch 24/60\n245/245 [==============================] - 21s 84ms/step - loss: 1.4759\nEpoch 25/60\n245/245 [==============================] - 21s 84ms/step - loss: 1.4589\nEpoch 26/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.4417\nEpoch 27/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.4245\nEpoch 28/60\n245/245 [==============================] - 20s 84ms/step - loss: 1.4071\nEpoch 29/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.3897\nEpoch 30/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.3724\nEpoch 31/60\n245/245 [==============================] - 21s 84ms/step - loss: 1.3560\nEpoch 32/60\n245/245 [==============================] - 20s 84ms/step - loss: 1.3383\nEpoch 33/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.3216\nEpoch 34/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.3047\nEpoch 35/60\n245/245 [==============================] - 20s 84ms/step - loss: 1.2880\nEpoch 36/60\n245/245 [==============================] - 20s 84ms/step - loss: 1.2718\nEpoch 37/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.2553\nEpoch 38/60\n245/245 [==============================] - 21s 84ms/step - loss: 1.2393\nEpoch 39/60\n245/245 [==============================] - 20s 84ms/step - loss: 1.2244\nEpoch 40/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.2100\nEpoch 41/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.1949\nEpoch 42/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.1816\nEpoch 43/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.1669\nEpoch 44/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.1542\nEpoch 45/60\n245/245 [==============================] - 21s 84ms/step - loss: 1.1406\nEpoch 46/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.1282\nEpoch 47/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.1169\nEpoch 48/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.1044\nEpoch 49/60\n245/245 [==============================] - 20s 84ms/step - loss: 1.0932\nEpoch 50/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.0821\nEpoch 51/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.0735\nEpoch 52/60\n245/245 [==============================] - 21s 84ms/step - loss: 1.0639\nEpoch 53/60\n245/245 [==============================] - 20s 84ms/step - loss: 1.0542\nEpoch 54/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.0439\nEpoch 55/60\n245/245 [==============================] - 21s 84ms/step - loss: 1.0356\nEpoch 56/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.0276\nEpoch 57/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.0199\nEpoch 58/60\n245/245 [==============================] - 20s 83ms/step - loss: 1.0127\nEpoch 59/60\n245/245 [==============================] - 21s 84ms/step - loss: 1.0045\nEpoch 60/60\n245/245 [==============================] - 20s 84ms/step - loss: 0.9985\n","output_type":"stream"}]},{"cell_type":"code","source":"tf.train.latest_checkpoint(lstm_dir_checkpoints)","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:45:26.030356Z","iopub.execute_input":"2023-03-05T06:45:26.030715Z","iopub.status.idle":"2023-03-05T06:45:26.041044Z","shell.execute_reply.started":"2023-03-05T06:45:26.030676Z","shell.execute_reply":"2023-03-05T06:45:26.040195Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"'./training_checkpoints_LSTM/checkpt_60'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"lstm_model = build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size=1)\nlstm_model.load_weights(tf.train.latest_checkpoint(lstm_dir_checkpoints))\nlstm_model.build(tf.TensorShape([1, None]))\n\nlstm_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:45:26.042330Z","iopub.execute_input":"2023-03-05T06:45:26.042796Z","iopub.status.idle":"2023-03-05T06:45:26.303013Z","shell.execute_reply.started":"2023-03-05T06:45:26.042758Z","shell.execute_reply":"2023-03-05T06:45:26.301507Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (1, None, 256)            35584     \n_________________________________________________________________\nlstm_2 (LSTM)                (1, None, 1024)           5246976   \n_________________________________________________________________\ndense_2 (Dense)              (1, None, 139)            142475    \n=================================================================\nTotal params: 5,425,035\nTrainable params: 5,425,035\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_text(model, start_string):\n    num_generate = 1000 #Number of characters to be generated\n\n    input_eval = [char2index[s] for s in start_string] #vectorising input\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    text_generated = []\n\n    # Low temperatures results in more predictable text.\n    # Higher temperatures results in more surprising text.\n    # Experiment to find the best setting.\n    temperature = 0.5\n\n    # Here batch size == 1\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # using a categorical distribution to predict the character returned by the model\n        predictions = predictions / temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        # We pass the predicted character as the next input to the model\n        # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(index2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:45:26.306892Z","iopub.execute_input":"2023-03-05T06:45:26.307154Z","iopub.status.idle":"2023-03-05T06:45:26.315897Z","shell.execute_reply.started":"2023-03-05T06:45:26.307125Z","shell.execute_reply":"2023-03-05T06:45:26.315195Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"#Testing\n#print(generate_text(lstm_model, start_string=u\"ROMEO: \"))","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:45:26.317073Z","iopub.execute_input":"2023-03-05T06:45:26.317440Z","iopub.status.idle":"2023-03-05T06:45:26.324550Z","shell.execute_reply.started":"2023-03-05T06:45:26.317389Z","shell.execute_reply":"2023-03-05T06:45:26.323758Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"#Prediction with User Input\nlstm_test = input(\"Enter your starting string: \")\nprint(generate_text(lstm_model, start_string=lstm_test))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-05T06:53:21.115751Z","iopub.execute_input":"2023-03-05T06:53:21.116138Z","iopub.status.idle":"2023-03-05T06:53:47.060144Z","shell.execute_reply.started":"2023-03-05T06:53:21.116102Z","shell.execute_reply":"2023-03-05T06:53:47.059306Z"},"trusted":true},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your starting string:  মৃত্যু স্পর্শ\n"},{"name":"stdout","text":"মৃত্যু স্পর্শে তাঁর আত্ম-অনুভব।\nমৃদু হেসে তরু ছায়া দিয়ে\nদোঁহারের\nপশ্চিমসীমায়,\nসে বিরাট নাহিকো তাহার সত্য মূল্য তার নেই।\nঅন্ধকারে দেখা যায় আরো সবার মতো\nসব গিয়াছে যে দুটি কথা —\nএত যাওয়া ভালোবেসেছিনু মনে\nভানু মরণের স্বর্ণমাঝারে\nনিঃশব্দ চরণে বরণ করিয়া প্রাণ।\nপশ্চাতে যে প্রেম নিমেষে নিমেষে বুদ্ বুদের মধ্যে কেউ কোথাও নেই।\nএমন সময় পাওয়া যেন\nমনটাতে মোর বুকের কাছে বারি বারে,\nসেখানে মিলেছে আঁধার ছায়ার তলে ;\nসে হিসাব রাখাল বেয়ে ঘেঁষে ছিঁড়ে পিঠে,\nসেখানে মাঠের পথের পথিক তুমি\nচুপে চুপে,\nসেই চলে গেল কত দিন একেবারে।\nসব চেয়ে সভা বসে বনের পাশে\nতবে কেন সে বাজে ভাবে,\nকেন এ কেমন করে।\nকোন্ খানে তার সেই\nমেয়েটির হাসি,\nঅমন করে আছিস কোথা।\nতোমার নীল আকাশের বাণী\nনিরন্তর স্তরে স্তরে আঁধার চকিতে\nনিম্নে সে দাঁড়ায়েছে দ্বারে গিয়া কী দিগন্তের মাঝে\nদুর্গম বন্ধুর সমুখে প্রবাহিয়া উঠে মাতি।\nসে যেন আমাদের প্রিয়ার আমারে\nসে কথা নিশিদিন ধরি মোর বাজিয়ে দেব!\nবিশ্বজনের পানে স্বপ্ন ভাঙো তটে ;\nমনে হল কাজে লাগিবে বাতি\nসে যেন কার তরে?\nসবার সাথে আমাদের সত্য নহে,\nসে কি অজানা ভাষা\nআমার মনের কথা বলা হবে।\nবাঁশির রব করি তাঁহার স্মৃতি,\nমুগ্ধ দৃষ্টি যার \n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}